{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Kaggle's Heart Failure dataset\n",
    "\n",
    "Required libraries for data wrangling, statistical analysis and plotting. ~~Plotnine was chosen for similarity to ggplot2 in R.~~ As Plotnine does not have a pairplot API for fast pairwise comparison, I'll switch to Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading, inspection and curation\n",
    "First, load the heart failure dataset supplied by Kaggle and perform some basic introspection on the overall shape of the data and the type of features it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()\n",
    "#sb.pairplot(df, hue=\"HeartDisease\", corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learns us that there are a total of 918 rows and 12 features. Several features have object datatype, but I would prefer category datatype. So for those features I'll check whether the number of distinct objects per feature is small. HeartDisease could be transformed to a boolean, perhaps ExerciseAngina nad FastingBS as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df[[\"Sex\",\"ChestPainType\",\"FastingBS\",\"RestingECG\",\"ExerciseAngina\"]]:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above suspicions are confirmed; the respective features have few distinct values each and some can be turned to a boolean. Below the colums are assigned their new data types and a check is done afterwards to assure the conversion was done correctly, such that every feature retains all of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to category\n",
    "#for col in df[[\"Sex\",\"ChestPainType\",\"RestingECG\"]]:\n",
    "#    df[col] = df[col].astype('category')\n",
    "# Convert to boolean\n",
    "#for col in df[[\"FastingBS\",\"HeartDisease\"]]:\n",
    "#    df[col] = df[col].astype('bool')\n",
    "df[\"HeartDisease\"] = df[\"HeartDisease\"].astype('bool')\n",
    "#df[\"ExerciseAngina\"] = df['ExerciseAngina'].replace({'N': 0, 'Y': 1})\n",
    "#df[\"ExerciseAngina\"] = df[\"ExerciseAngina\"].astype('bool')\n",
    "print(df.dtypes)\n",
    "for col in df[[\"Sex\",\"ChestPainType\",\"FastingBS\",\"RestingECG\",\"ExerciseAngina\",\"HeartDisease\"]]:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative analysis\n",
    "\n",
    "There are a variety of interesting feature combinations for an initial explorative analysis, to give some insight on general feature value distributions and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binned 5y range age for later\n",
    "# cut_bins = []\n",
    "# cut_labels = []\n",
    "# for i in range(0,20):\n",
    "#     cut_bins.append(i*5)\n",
    "#     cut_labels.append(str((i*5)-5)+\"-\"+str(i*5))\n",
    "# cut_labels.remove(\"-5-0\") # nr of labels should be bins-1\n",
    "# df[\"Bin_Age\"] = pd.cut(df[\"Age\"], bins=cut_bins, labels=cut_labels)\n",
    "# df[\"Bin_Age\"] = df[\"Bin_Age\"].astype(\"category\")\n",
    "\n",
    "# Distribution of HD per Sex, as counts and percentage of total nr of people\n",
    "df_dist = (\n",
    "    df.groupby('Sex', as_index=True)\n",
    "        .agg(HeartDisease=('HeartDisease', 'sum'),\n",
    "             Nr_of_people=('HeartDisease', 'count'),\n",
    "             HD_pct=('HeartDisease', 'mean'))\n",
    ")\n",
    "df_dist[\"HD_pct\"] *= 100\n",
    "print(df_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Male vs Female\n",
    "sb.countplot(x=\"Sex\",data=df)\n",
    "\n",
    "# Plot distribution of Male/Female to Age, with median age\n",
    "g = sb.FacetGrid(df, col=\"Sex\", height=3.5, aspect=1)\n",
    "g.map(sb.histplot, \"Age\")\n",
    "for axes in g.axes.flat:\n",
    "    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb.pairplot(df, hue=\"HeartDisease\", corner=True,\n",
    "# vars = [\"Age\",\"RestingBP\",\"Cholesterol\",\"MaxHR\",\"Oldpeak\"],\n",
    "# plot_kws={\"alpha\":0.4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df.loc[df[\"Sex\"] == \"M\"]\n",
    "df_f = df.loc[df[\"Sex\"] == \"F\"]\n",
    "for df_x in [df_m,df_f]:\n",
    "    sb.pairplot(df_x, hue=\"HeartDisease\", corner=True,\n",
    "    vars = [\"Age\",\"RestingBP\",\"Cholesterol\",\"MaxHR\",\"Oldpeak\"],\n",
    "    plot_kws={\"alpha\":0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple binary classification NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "for lab in [\"Sex\",\"ChestPainType\",\"RestingECG\",\"ExerciseAngina\",\"ST_Slope\"]:\n",
    "    df[lab] = le.fit_transform(df[lab])\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input/output\n",
    "X = df.iloc[:, 0:-1] # All features minus HeartDisease\n",
    "y = df.iloc[:, -1] # HeartDisease\n",
    "\n",
    "# Define train/test sizes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "# Standardization input feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Parameters of the model\n",
    "torch.manual_seed(0)\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#        False       0.87      0.80      0.83       125\n",
    "#         True       0.87      0.92      0.89       178\n",
    "# EPOCHS = 11\n",
    "# BATCH_SIZE = 512\n",
    "# LEARNING_RATE = 0.01\n",
    "\n",
    "# recall 0.77 0.90\n",
    "# EPOCHS = 10\n",
    "# BATCH_SIZE = 8\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# recall 0.86 0.82\n",
    "# EPOCHS = 11\n",
    "# BATCH_SIZE = 12\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# # 0.90\n",
    "# EPOCHS = 10\n",
    "# BATCH_SIZE = 8\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# Dataloaders\n",
    "## train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "## test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test))\n",
    "\n",
    "# Initialize loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(11, 64) # 11 input features in our dataset\n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU pls\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Optimizer / loss function\n",
    "model = BinaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec4b7ae1099cca5b3a474f3b2d2de3a37939891147ee87c8fe4cb1962faad7a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
